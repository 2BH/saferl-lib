# SafetyAntVelocity with CSAC-LB Configuration
# This configuration combines environment, algorithm, and training settings
# for the SafetyAntVelocity environment using CSAC-LB algorithm
# Based on: "Constrained Reinforcement Learning with Smoothed Log Barrier Function" (TMLR 2025)

# =============================================================================
# EXPERIMENT SETTINGS
# =============================================================================
seed: 0
device: "cuda"
verbose: 1
save_freq: 100000
eval_freq: 1000
save_video_freq: 100000 
save_video: true
num_eval_episodes: 1
norm_obs: false
norm_act: true
norm_reward: true
norm_cost: false
num_env: 10
use_multi_process: false

# =============================================================================
# ENVIRONMENT CONFIGURATION
# =============================================================================
env:
  total_timesteps: 3000000
  episode_len: 1000
  train_env:
    env_name: SafetyAntVelocity-v1
    env_kwargs: 
      camera_id: 0
    num_env: ${num_env}
  
  eval_env:
    env_name: SafetyAntVelocity-v1
    env_kwargs: 
      render_mode: rgb_array
      camera_id: 0

# =============================================================================
# ALGORITHM CONFIGURATION - CSAC-LB
# =============================================================================
algorithm:
  model:
    _target_: saferl.algorithms.csac_lb.CSAC_LB
    _convert_: partial
    
    # Core SAC parameters
    buffer_size: 1000000
    gamma: [0.99, 0.99]  # [reward_gamma, cost_gamma]
    learning_rate: 3e-4
    learning_starts: 100
    batch_size: 256
    tau: 0.005
    gradient_steps: 1
    train_freq: 1
    
    # CSAC-LB specific parameters
    cost_constraint: [5.0]  # Cost limit for constraint return
    lower_bound: 0.1        # Lower bound for log barrier function
    
    # Entropy regularization
    ent_coef: "auto"
    target_entropy: "auto"
    target_update_interval: 1
    
    # Policy network architecture
    policy_kwargs:
      net_arch: [256, 256]
      share_features_extractor: false
      ortho_init: false
    
    # Training settings
    verbose: ${verbose}
    device: ${device}
    seed: ${seed}
    
    # Optional parameters (uncomment to use)
    # use_sde: false
    # sde_sample_freq: 10
    # use_sde_at_warmup: false
    # replay_buffer_kwargs:
    #   track_transition_type_stats: true
  
  algorithm_name: csac_lb
  policy_class: SACwithCostPolicy
  noise: null

# =============================================================================
# CALLBACK CONFIGURATION
# =============================================================================
callback:
  on_step_callback:
    _target_: saferl.common.callbacks.EvalCallback
    eval_env: ${env.eval_env}
    eval_freq: ${eval_freq}
    n_eval_episodes: ${num_eval_episodes}
    log_path: ${hydra:runtime.cwd}/logs/
    save_path: ${hydra:runtime.cwd}/models/
    save_freq: ${save_freq}
    save_video_freq: ${save_video_freq}
    save_video: ${save_video}
    deterministic: true
    render: false
    verbose: 1

# =============================================================================
# HYDRA CONFIGURATION
# =============================================================================
hydra:
  mode: RUN
  job:
    chdir: true
  run:
    dir: ./exp/local/${now:%Y.%m.%d}/Benchmark/${env.train_env.env_name}/${algorithm.algorithm_name}/${seed}_${now:%H%M%S}/Seed${seed}_Cost${algorithm.model.cost_constraint}
  sweep:
    dir: ./exp/local/${now:%Y.%m.%d}/Benchmark/${env.train_env.env_name}/${algorithm.algorithm_name}/${seed}_${now:%H%M%S}
    subdir: ${hydra.job.num}_Seed${seed}_Cost${algorithm.model.cost_constraint}
