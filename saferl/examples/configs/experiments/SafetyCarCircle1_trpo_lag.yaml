# SafetyCarCircle1 with TRPO-Lag Configuration
# This configuration combines environment, algorithm, and training settings
# for the SafetyCarCircle1 environment using TRPO-Lag algorithm

# =============================================================================
# EXPERIMENT SETTINGS
# =============================================================================
seed: 0
device: "cuda"
verbose: 1
save_freq: 100000
eval_freq: 1000
save_video_freq: 100000 
save_video: true
num_eval_episodes: 1
norm_obs: false
norm_act: true
norm_reward: true
norm_cost: false
num_env: 10
use_multi_process: false

# =============================================================================
# ENVIRONMENT CONFIGURATION
# =============================================================================
env:
  total_timesteps: 3000000
  episode_len: 500
  train_env:
    env_name: SafetyCarCircle1-v0
    env_kwargs: 
      camera_id: 1
    num_env: ${num_env}
  
  eval_env:
    env_name: SafetyCarCircle1-v0
    env_kwargs: 
      render_mode: rgb_array
      camera_id: 1

# =============================================================================
# ALGORITHM CONFIGURATION
# =============================================================================
algorithm:
  model:
    _target_: saferl.algorithms.trpo_lag.TRPO_LAG
    _convert_: partial
    
    # Core TRPO parameters
    gamma: [0.99, 0.99]  # [reward_gamma, cost_gamma]
    gae_lambda: [0.95, 0.95]
    learning_rate: 3e-4
    n_steps: 50  # rollout length per env
    batch_size: 256
    n_critic_updates: 10
    cg_max_steps: 15
    line_search_max_iter: 15
    normalize_advantage: true
    reset_on_rollout_end: true
    
    # Safety constraint parameters
    cost_constraint: [25.0]  # Cost limit for constraint return
    inital_lagrange_multipliers: [0.001]
    lagrange_multiplier_learning_rate: 0.005
    
    # Policy network architecture
    policy_kwargs:
      net_arch: [64, 64]
      share_features_extractor: false
      rpo_perturbance_alpha: 0.0
      ortho_init: false
    
    # Training settings
    stats_window_size: 50
    verbose: ${verbose}
    device: ${device}
    seed: ${seed}
  
  algorithm_name: trpo_lag
  policy_class: ACWithCostPolicy
  noise: null

# =============================================================================
# CALLBACK CONFIGURATION
# =============================================================================
callback:
  on_step_callback:
    _target_: saferl.common.callbacks.EvalCallback
    eval_env: ${env.eval_env}
    eval_freq: ${eval_freq}
    n_eval_episodes: ${num_eval_episodes}
    log_path: ${hydra:runtime.cwd}/logs/
    save_path: ${hydra:runtime.cwd}/models/
    save_freq: ${save_freq}
    save_video_freq: ${save_video_freq}
    save_video: ${save_video}
    deterministic: true
    render: false
    verbose: 1

# =============================================================================
# HYDRA CONFIGURATION
# =============================================================================
hydra:
  mode: RUN
  job:
    chdir: true
  run:
    dir: ./exp/local/${now:%Y.%m.%d}/Benchmark/${env.train_env.env_name}/${algorithm.algorithm_name}/${seed}_${now:%H%M%S}/Seed${seed}_Cost${algorithm.model.cost_constraint}
  sweep:
    dir: ./exp/local/${now:%Y.%m.%d}/Benchmark/${env.train_env.env_name}/${algorithm.algorithm_name}/${seed}_${now:%H%M%S}
    subdir: ${hydra.job.num}_Seed${seed}_Cost${algorithm.model.cost_constraint}
